{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectra Encoder: Transformer\n",
    "\n",
    "Primary reference: https://www.nature.com/articles/s42004-023-00932-3#Sec19\n",
    "\n",
    "Using this paper as a framework, the purpose of this transformer is to take input GC-MS spectral data and output embeddings to be passed to the SMILES decoder. The reference used images of GC-MS data and implemented a CNN; we intend to use a transformer instead.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplemental references:\n",
    "https://jalammar.github.io/illustrated-transformer/ (Illustrated overview of Transformer function)\n",
    "\n",
    "https://nlp.seas.harvard.edu/2018/04/03/attention.html (Harvard coding annotation of original Transformation paper)\n",
    "\n",
    "https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch (Datacamp Transformer tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook overview:\n",
    "1. Define model building blocks\n",
    "2. Encoding\n",
    "3. Decoding\n",
    "4. Training\n",
    "5. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# finding unique characters in the SMILES column of training data \n",
    "\n",
    "unique_characters = set() \n",
    "\n",
    "with open('dataset/filtered_gc_spec.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)  \n",
    "    for row in reader:\n",
    "        for char in row[\"SMILES\"]:\n",
    "            unique_characters.add(char)  # Add each character to the set\n",
    "\n",
    "print(len(unique_characters))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517627\n"
     ]
    }
   ],
   "source": [
    "# finding unique tuples in the spectral training data\n",
    "unique_tuples = set()  \n",
    "\n",
    "with open('dataset/filtered_gc_spec.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        spectrum_data = row[\"Spectrum\"]\n",
    "        tuples = spectrum_data.split() \n",
    "        for tup in tuples:\n",
    "            if ':' in tup and tup.count(':') == 1: \n",
    "                unique_tuples.add(tup)\n",
    "\n",
    "print(len(unique_tuples))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to go from a \"vocabulary\" of 517627 unique tuples to 45 unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input and output datasets for training\n",
    "\n",
    "# load dataset\n",
    "data = pd.read_csv('dataset/filtered_gc_spec.csv')\n",
    "input_MS = pd.Series(data[\"Spectrum\"])\n",
    "output_SMILES = pd.Series(data[\"SMILES\"])\n",
    "\n",
    "assert len(input_MS)==len(output_SMILES) #sanity check to ensure correct loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GC-MS Spectra for input: 18792\n",
      "Number of SMILES sequences for output: 18792\n"
     ]
    }
   ],
   "source": [
    "#filter input by length of SMILES (<77 as per SMILES encoder)\n",
    "\n",
    "output_SMILES_filtered = output_SMILES[output_SMILES.str.len() < 77]\n",
    "\n",
    "#filter input by the same indices\n",
    "input_MS_filtered = input_MS.loc[output_SMILES_filtered.index]\n",
    "\n",
    "assert len(input_MS_filtered)==len(output_SMILES_filtered) #sanity check to ensure correct filtering\n",
    "\n",
    "print(f\"Number of GC-MS Spectra for input: {len(input_MS_filtered)}\")\n",
    "print(f\"Number of SMILES sequences for output: {len(output_SMILES_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the multihead attention class \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "       \n",
    "        # model parameters\n",
    "        self.d_model = d_model \n",
    "        self.num_heads = num_heads # number of attention heads\n",
    "        self.d_k = d_model // num_heads # dimension of each head's key, query, and value\n",
    "        \n",
    "        #transformation of inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # apply mask if necessary (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # softmax to convert to probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # output = matrix multiplication of attention probabilities x values\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # reshape the input to have num_heads for multi-head attention\n",
    "        #allows us to process multiple heads at the same time: parallel computing\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # apply linear transformations to inputs, split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # scaled dot-product attention on split heads - mask\n",
    "        # default mask is none - we can change this if we want to mask out certain values\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # combine heads and apply output linear transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each layers contains feed-forward network - applied to each position\n",
    "# two linear transformations and a reLU activation\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff): #inputs - dimensions and inner-layer dimensions\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings class converts input/output tokens to vectors specified by the dimensions of our model\n",
    "#softmax converts the output to probabilities\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positional encoding is used to inject token position info into the input\n",
    "#otherwise transformer has no info about token position in the input sequence\n",
    "#essentially uses offset sin/cos graphs based on position. freq/offset is different for each dimension\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    #adds positional info to the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining encoder layer for class\n",
    "#steps: multiattention, position feed forward, 2x layer normalization, dropout\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads) #self attention mechanism\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff) #attention output appended to original input\n",
    "        self.norm1 = nn.LayerNorm(d_model) #normalization\n",
    "        self.norm2 = nn.LayerNorm(d_model) #normalization\n",
    "        self.dropout = nn.Dropout(dropout) #dropout\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Transformer class\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model) #takes in source vocab size \n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length) #max sequence length for positional encoding\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout) #dropout\n",
    "\n",
    "        def forward(self, src, src_mask=None):\n",
    "            x = self.encoder_embedding(src)  \n",
    "            x = self.positional_encoding(x)  \n",
    "        \n",
    "            for layer in self.encoder_layers:\n",
    "                x = layer(x, src_mask)  \n",
    "\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            return x #output is fully embedded spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000 #need to specify\n",
    "tgt_vocab_size = 5000 #need to specify\n",
    "d_model = 512 #from original paper\n",
    "num_heads = 8 #from original paper\n",
    "num_layers = 6 #from original paper\n",
    "d_ff = 2048 #from original paper\n",
    "max_seq_length = 77\n",
    "dropout = 0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
