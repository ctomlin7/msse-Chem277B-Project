{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectra Encoder: Transformer\n",
    "\n",
    "Primary reference: https://www.nature.com/articles/s42004-023-00932-3#Sec19\n",
    "\n",
    "Using this paper as a framework, the purpose of this transformer is to take input GC-MS spectral data and output embeddings to be passed to the SMILES decoder. The reference used images of GC-MS data and implemented a CNN; we intend to use a transformer instead.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplemental references:\n",
    "https://jalammar.github.io/illustrated-transformer/ (Illustrated overview of Transformer function)\n",
    "\n",
    "https://nlp.seas.harvard.edu/2018/04/03/attention.html (Harvard coding annotation of original Transformation paper)\n",
    "\n",
    "https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch (Datacamp Transformer tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook overview:\n",
    "1. Define model building blocks\n",
    "2. Encoding\n",
    "3. Decoding\n",
    "4. Training\n",
    "5. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# finding unique characters in the SMILES column of training data \n",
    "\n",
    "unique_characters = set() \n",
    "\n",
    "with open('dataset/filtered_gc_spec.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)  \n",
    "    for row in reader:\n",
    "        for char in row[\"SMILES\"]:\n",
    "            unique_characters.add(char)  # Add each character to the set\n",
    "\n",
    "print(len(unique_characters))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517627\n"
     ]
    }
   ],
   "source": [
    "# finding unique tuples in the spectral training data\n",
    "unique_tuples = set()  \n",
    "\n",
    "with open('dataset/filtered_gc_spec.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        spectrum_data = row[\"Spectrum\"]\n",
    "        tuples = spectrum_data.split() \n",
    "        for tup in tuples:\n",
    "            if ':' in tup and tup.count(':') == 1: \n",
    "                unique_tuples.add(tup)\n",
    "\n",
    "print(len(unique_tuples))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to go from a \"vocabulary\" of 517627 unique tuples to 45 unique characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MS Spectra for input: 100\n",
      "Number of SMILES sequences for output: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BartTokenizer  \n",
    "\n",
    "# BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('dataset/filtered_gc_spec.csv')\n",
    "input_MS = pd.Series(data[\"Spectrum\"][:100])\n",
    "output_SMILES = pd.Series(data[\"SMILES\"][:100])\n",
    "\n",
    "assert len(input_MS) == len(output_SMILES)  # sanity check to ensure correct loading\n",
    "\n",
    "# filter input by length of SMILES (<77 as per SMILES encoder)\n",
    "output_SMILES_filtered = output_SMILES[output_SMILES.str.len() < 77]\n",
    "input_MS_filtered = input_MS.loc[output_SMILES_filtered.index]\n",
    "\n",
    "assert len(input_MS_filtered) == len(output_SMILES_filtered)  # sanity check to ensure correct filtering\n",
    "\n",
    "print(f\"Number of MS Spectra for input: {len(input_MS_filtered)}\")\n",
    "print(f\"Number of SMILES sequences for output: {len(output_SMILES_filtered)}\")\n",
    "\n",
    "smiles_list = output_SMILES_filtered.tolist()\n",
    "\n",
    "# tokenize SMILES data using BART tokenizer\n",
    "tokenized_smiles = [tokenizer.encode(smiles, add_special_tokens=True, padding='max_length', truncation=True, max_length=77) for smiles in smiles_list]\n",
    "\n",
    "smiles_tensor = torch.tensor(tokenized_smiles, dtype=torch.long)\n",
    "\n",
    "# set max length of MS to 200\n",
    "max_length = 200\n",
    "\n",
    "# converting MS data to tensor\n",
    "def spec_2_tensor(spectrum, max_length):\n",
    "    spectrum_tuples = [(float(mz), float(intensity)) for mz, intensity in (item.split(\":\") for item in spectrum.split())]\n",
    "    return spectrum_tuples[:max_length] + [(0, 0)] * (max_length - len(spectrum_tuples))  # padding as zeroes\n",
    "\n",
    "input_MS_data = input_MS_filtered.apply(lambda x: spec_2_tensor(x, max_length))\n",
    "ms_tensor = torch.tensor(input_MS_data.tolist(), dtype=torch.float32)\n",
    "\n",
    "# linear layer to map from 2 features to vocab size\n",
    "src_vocab_size = 5000 #should be 500,000, but my computer can't handle it\n",
    "linear_layer = nn.Linear(2, src_vocab_size)\n",
    "\n",
    "# flatten, transform, and reshape back\n",
    "ms_tensor_flat = ms_tensor.view(-1, 2)  # flatten for batch processing\n",
    "ms_tensor_transformed = linear_layer(ms_tensor_flat)\n",
    "ms_tensor_indices = ms_tensor_transformed.argmax(dim=1)  # select the index with the highest value\n",
    "ms_tensor_indices = ms_tensor_indices.view(len(input_MS_filtered), max_length)  # reshape back\n",
    "\n",
    "# train/test split\n",
    "ms_train, ms_test, smiles_train, smiles_test = train_test_split(ms_tensor_indices.numpy(), smiles_tensor.numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# convert back to tensors\n",
    "ms_train = torch.tensor(ms_train, dtype=torch.long)\n",
    "ms_test = torch.tensor(ms_test, dtype=torch.long)\n",
    "smiles_train = torch.tensor(smiles_train, dtype=torch.long)\n",
    "smiles_test = torch.tensor(smiles_test, dtype=torch.long)\n",
    "\n",
    "# create datasets and dataloaders\n",
    "batch_size_train = 10 #keep this low to avoid kernel crashing\n",
    "batch_size_test = 10\n",
    "\n",
    "train_dataset = TensorDataset(ms_train, smiles_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(ms_test, smiles_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Epoch 1 -------------------------\n",
      "Train Accuracy: 0.0000, Precision: 0.4894, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.3529, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.2727, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.2273, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.6970, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.6286, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.4783, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.4800, F1: 0.0000\n",
      "Validation Accuracy: 0.0000, Precision: 0.9565, F1: 0.0000\n",
      "Validation Accuracy: 0.0000, Precision: 0.9565, F1: 0.0000\n",
      "Train Loss: 10.5278, Validation Loss: 10.0218\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 2 -------------------------\n",
      "Train Accuracy: 0.0000, Precision: 0.3958, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.4792, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.4318, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.5227, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.5893, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.4545, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.5000, F1: 0.0000\n",
      "Train Accuracy: 0.0000, Precision: 0.4773, F1: 0.0000\n",
      "Validation Accuracy: 0.0000, Precision: 0.9231, F1: 0.0000\n",
      "Validation Accuracy: 0.0000, Precision: 0.9500, F1: 0.0000\n",
      "Train Loss: 9.9269, Validation Loss: 9.6351\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 3 -------------------------\n",
      "Train Accuracy: 0.0000, Precision: 0.5111, F1: 0.0000\n",
      "Train Accuracy: 0.0143, Precision: 0.4681, F1: 0.0008\n",
      "Train Accuracy: 0.1519, Precision: 0.4690, F1: 0.0082\n",
      "Train Accuracy: 0.3442, Precision: 0.6120, F1: 0.0116\n",
      "Train Accuracy: 0.5221, Precision: 0.6034, F1: 0.0201\n",
      "Train Accuracy: 0.6260, Precision: 0.7034, F1: 0.0280\n",
      "Train Accuracy: 0.6597, Precision: 0.6680, F1: 0.0260\n",
      "Train Accuracy: 0.6766, Precision: 0.8562, F1: 0.0352\n",
      "Validation Accuracy: 0.6870, Precision: 0.9844, F1: 0.0407\n",
      "Validation Accuracy: 0.7052, Precision: 0.9877, F1: 0.0345\n",
      "Train Loss: 9.5598, Validation Loss: 9.2434\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 4 -------------------------\n",
      "Train Accuracy: 0.6818, Precision: 0.8033, F1: 0.0301\n",
      "Train Accuracy: 0.6584, Precision: 0.9362, F1: 0.0378\n",
      "Train Accuracy: 0.6675, Precision: 0.9467, F1: 0.0320\n",
      "Train Accuracy: 0.6805, Precision: 0.9861, F1: 0.0352\n",
      "Train Accuracy: 0.6740, Precision: 0.9891, F1: 0.0268\n",
      "Train Accuracy: 0.7078, Precision: 0.9854, F1: 0.0414\n",
      "Train Accuracy: 0.5987, Precision: 0.9826, F1: 0.0326\n",
      "Train Accuracy: 0.7610, Precision: 0.9891, F1: 0.0393\n",
      "Validation Accuracy: 0.7130, Precision: 0.9880, F1: 0.0347\n",
      "Validation Accuracy: 0.6792, Precision: 0.9840, F1: 0.0404\n",
      "Train Loss: 9.1605, Validation Loss: 8.8034\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 5 -------------------------\n",
      "Train Accuracy: 0.7169, Precision: 0.9858, F1: 0.0418\n",
      "Train Accuracy: 0.7026, Precision: 0.9904, F1: 0.0266\n",
      "Train Accuracy: 0.6844, Precision: 0.9863, F1: 0.0353\n",
      "Train Accuracy: 0.7000, Precision: 0.9870, F1: 0.0358\n",
      "Train Accuracy: 0.6922, Precision: 0.9872, F1: 0.0341\n",
      "Train Accuracy: 0.6208, Precision: 0.9810, F1: 0.0383\n",
      "Train Accuracy: 0.6221, Precision: 0.9790, F1: 0.0426\n",
      "Train Accuracy: 0.7000, Precision: 0.9850, F1: 0.0412\n",
      "Validation Accuracy: 0.7091, Precision: 0.9868, F1: 0.0377\n",
      "Validation Accuracy: 0.6831, Precision: 0.9856, F1: 0.0369\n",
      "Train Loss: 8.7137, Validation Loss: 8.3162\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 6 -------------------------\n",
      "Train Accuracy: 0.6883, Precision: 0.9844, F1: 0.0408\n",
      "Train Accuracy: 0.6818, Precision: 0.9841, F1: 0.0405\n",
      "Train Accuracy: 0.6571, Precision: 0.9844, F1: 0.0361\n",
      "Train Accuracy: 0.6442, Precision: 0.9877, F1: 0.0270\n",
      "Train Accuracy: 0.6844, Precision: 0.9842, F1: 0.0406\n",
      "Train Accuracy: 0.6195, Precision: 0.9819, F1: 0.0364\n",
      "Train Accuracy: 0.7234, Precision: 0.9885, F1: 0.0350\n",
      "Train Accuracy: 0.7403, Precision: 0.9876, F1: 0.0405\n",
      "Validation Accuracy: 0.6948, Precision: 0.9861, F1: 0.0373\n",
      "Validation Accuracy: 0.6974, Precision: 0.9856, F1: 0.0391\n",
      "Train Loss: 8.2228, Validation Loss: 7.7816\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 7 -------------------------\n",
      "Train Accuracy: 0.7390, Precision: 0.9876, F1: 0.0405\n",
      "Train Accuracy: 0.6974, Precision: 0.9868, F1: 0.0357\n",
      "Train Accuracy: 0.6662, Precision: 0.9841, F1: 0.0381\n",
      "Train Accuracy: 0.7273, Precision: 0.9876, F1: 0.0383\n",
      "Train Accuracy: 0.7039, Precision: 0.9852, F1: 0.0413\n",
      "Train Accuracy: 0.6130, Precision: 0.9806, F1: 0.0380\n",
      "Train Accuracy: 0.6481, Precision: 0.9886, F1: 0.0254\n",
      "Train Accuracy: 0.6442, Precision: 0.9852, F1: 0.0326\n",
      "Validation Accuracy: 0.7169, Precision: 0.9871, F1: 0.0380\n",
      "Validation Accuracy: 0.6753, Precision: 0.9852, F1: 0.0366\n",
      "Train Loss: 7.6611, Validation Loss: 7.1317\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 8 -------------------------\n",
      "Train Accuracy: 0.7208, Precision: 0.9884, F1: 0.0349\n",
      "Train Accuracy: 0.6519, Precision: 0.9842, F1: 0.0359\n",
      "Train Accuracy: 0.6961, Precision: 0.9848, F1: 0.0410\n",
      "Train Accuracy: 0.6818, Precision: 0.9841, F1: 0.0405\n",
      "Train Accuracy: 0.6818, Precision: 0.9882, F1: 0.0300\n",
      "Train Accuracy: 0.6377, Precision: 0.9835, F1: 0.0354\n",
      "Train Accuracy: 0.6766, Precision: 0.9830, F1: 0.0425\n",
      "Train Accuracy: 0.6922, Precision: 0.9829, F1: 0.0455\n",
      "Validation Accuracy: 0.7416, Precision: 0.9888, F1: 0.0370\n",
      "Validation Accuracy: 0.6506, Precision: 0.9825, F1: 0.0394\n",
      "Train Loss: 6.9931, Validation Loss: 6.4002\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 9 -------------------------\n",
      "Train Accuracy: 0.6506, Precision: 0.9834, F1: 0.0375\n",
      "Train Accuracy: 0.7000, Precision: 0.9850, F1: 0.0412\n",
      "Train Accuracy: 0.6701, Precision: 0.9857, F1: 0.0349\n",
      "Train Accuracy: 0.6597, Precision: 0.9897, F1: 0.0241\n",
      "Train Accuracy: 0.6844, Precision: 0.9842, F1: 0.0406\n",
      "Train Accuracy: 0.6831, Precision: 0.9862, F1: 0.0353\n",
      "Train Accuracy: 0.7325, Precision: 0.9878, F1: 0.0384\n",
      "Train Accuracy: 0.6584, Precision: 0.9837, F1: 0.0378\n",
      "Validation Accuracy: 0.7156, Precision: 0.9865, F1: 0.0397\n",
      "Validation Accuracy: 0.6766, Precision: 0.9853, F1: 0.0367\n",
      "Train Loss: 6.2396, Validation Loss: 5.5803\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 10 -------------------------\n",
      "Train Accuracy: 0.7052, Precision: 0.9845, F1: 0.0435\n",
      "Train Accuracy: 0.6351, Precision: 0.9865, F1: 0.0288\n",
      "Train Accuracy: 0.6662, Precision: 0.9861, F1: 0.0333\n",
      "Train Accuracy: 0.7208, Precision: 0.9873, F1: 0.0381\n",
      "Train Accuracy: 0.6857, Precision: 0.9843, F1: 0.0407\n",
      "Train Accuracy: 0.6714, Precision: 0.9844, F1: 0.0383\n",
      "Train Accuracy: 0.6870, Precision: 0.9826, F1: 0.0452\n",
      "Train Accuracy: 0.6675, Precision: 0.9849, F1: 0.0364\n",
      "Validation Accuracy: 0.6961, Precision: 0.9855, F1: 0.0391\n",
      "Validation Accuracy: 0.6961, Precision: 0.9873, F1: 0.0342\n",
      "Train Loss: 5.4320, Validation Loss: 4.7775\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 11 -------------------------\n",
      "Train Accuracy: 0.7052, Precision: 0.9905, F1: 0.0267\n",
      "Train Accuracy: 0.6649, Precision: 0.9814, F1: 0.0444\n",
      "Train Accuracy: 0.6571, Precision: 0.9844, F1: 0.0361\n",
      "Train Accuracy: 0.6675, Precision: 0.9804, F1: 0.0471\n",
      "Train Accuracy: 0.6753, Precision: 0.9845, F1: 0.0384\n",
      "Train Accuracy: 0.6857, Precision: 0.9863, F1: 0.0354\n",
      "Train Accuracy: 0.6766, Precision: 0.9846, F1: 0.0384\n",
      "Train Accuracy: 0.7065, Precision: 0.9867, F1: 0.0376\n",
      "Validation Accuracy: 0.6922, Precision: 0.9860, F1: 0.0372\n",
      "Validation Accuracy: 0.7000, Precision: 0.9850, F1: 0.0412\n",
      "Train Loss: 4.7411, Validation Loss: 4.2633\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 12 -------------------------\n",
      "Train Accuracy: 0.7234, Precision: 0.9862, F1: 0.0420\n",
      "Train Accuracy: 0.6286, Precision: 0.9845, F1: 0.0322\n",
      "Train Accuracy: 0.6610, Precision: 0.9846, F1: 0.0362\n",
      "Train Accuracy: 0.7039, Precision: 0.9882, F1: 0.0330\n",
      "Train Accuracy: 0.7013, Precision: 0.9900, F1: 0.0275\n",
      "Train Accuracy: 0.6961, Precision: 0.9855, F1: 0.0391\n",
      "Train Accuracy: 0.6818, Precision: 0.9841, F1: 0.0405\n",
      "Train Accuracy: 0.6429, Precision: 0.9830, F1: 0.0373\n",
      "Validation Accuracy: 0.6429, Precision: 0.9830, F1: 0.0373\n",
      "Validation Accuracy: 0.7494, Precision: 0.9886, F1: 0.0389\n",
      "Train Loss: 4.3828, Validation Loss: 4.0840\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 13 -------------------------\n",
      "Train Accuracy: 0.6909, Precision: 0.9886, F1: 0.0303\n",
      "Train Accuracy: 0.7091, Precision: 0.9868, F1: 0.0377\n",
      "Train Accuracy: 0.6831, Precision: 0.9856, F1: 0.0369\n",
      "Train Accuracy: 0.6714, Precision: 0.9878, F1: 0.0298\n",
      "Train Accuracy: 0.6948, Precision: 0.9855, F1: 0.0390\n",
      "Train Accuracy: 0.6701, Precision: 0.9843, F1: 0.0382\n",
      "Train Accuracy: 0.6688, Precision: 0.9834, F1: 0.0401\n",
      "Train Accuracy: 0.6506, Precision: 0.9841, F1: 0.0358\n",
      "Validation Accuracy: 0.6753, Precision: 0.9838, F1: 0.0403\n",
      "Validation Accuracy: 0.7169, Precision: 0.9882, F1: 0.0348\n",
      "Train Loss: 4.2634, Validation Loss: 4.0273\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 14 -------------------------\n",
      "Train Accuracy: 0.6429, Precision: 0.9872, F1: 0.0280\n",
      "Train Accuracy: 0.7221, Precision: 0.9854, F1: 0.0441\n",
      "Train Accuracy: 0.6740, Precision: 0.9819, F1: 0.0447\n",
      "Train Accuracy: 0.6623, Precision: 0.9839, F1: 0.0379\n",
      "Train Accuracy: 0.6416, Precision: 0.9857, F1: 0.0313\n",
      "Train Accuracy: 0.6818, Precision: 0.9823, F1: 0.0450\n",
      "Train Accuracy: 0.6649, Precision: 0.9848, F1: 0.0363\n",
      "Train Accuracy: 0.7494, Precision: 0.9881, F1: 0.0408\n",
      "Validation Accuracy: 0.7169, Precision: 0.9871, F1: 0.0380\n",
      "Validation Accuracy: 0.6753, Precision: 0.9859, F1: 0.0351\n",
      "Train Loss: 4.2162, Validation Loss: 3.9973\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 15 -------------------------\n",
      "Train Accuracy: 0.6987, Precision: 0.9841, F1: 0.0433\n",
      "Train Accuracy: 0.6675, Precision: 0.9815, F1: 0.0445\n",
      "Train Accuracy: 0.6377, Precision: 0.9883, F1: 0.0251\n",
      "Train Accuracy: 0.7130, Precision: 0.9870, F1: 0.0378\n",
      "Train Accuracy: 0.6675, Precision: 0.9842, F1: 0.0381\n",
      "Train Accuracy: 0.6688, Precision: 0.9849, F1: 0.0364\n",
      "Train Accuracy: 0.6597, Precision: 0.9821, F1: 0.0418\n",
      "Train Accuracy: 0.7260, Precision: 0.9890, F1: 0.0336\n",
      "Validation Accuracy: 0.7286, Precision: 0.9877, F1: 0.0383\n",
      "Validation Accuracy: 0.6636, Precision: 0.9840, F1: 0.0380\n",
      "Train Loss: 4.1889, Validation Loss: 3.9725\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 16 -------------------------\n",
      "Train Accuracy: 0.6377, Precision: 0.9842, F1: 0.0339\n",
      "Train Accuracy: 0.7065, Precision: 0.9860, F1: 0.0394\n",
      "Train Accuracy: 0.6468, Precision: 0.9859, F1: 0.0314\n",
      "Train Accuracy: 0.7000, Precision: 0.9885, F1: 0.0317\n",
      "Train Accuracy: 0.6597, Precision: 0.9887, F1: 0.0265\n",
      "Train Accuracy: 0.6909, Precision: 0.9828, F1: 0.0454\n",
      "Train Accuracy: 0.7234, Precision: 0.9868, F1: 0.0400\n",
      "Train Accuracy: 0.6740, Precision: 0.9819, F1: 0.0447\n",
      "Validation Accuracy: 0.6909, Precision: 0.9866, F1: 0.0355\n",
      "Validation Accuracy: 0.7013, Precision: 0.9858, F1: 0.0393\n",
      "Train Loss: 4.1635, Validation Loss: 3.9492\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 17 -------------------------\n",
      "Train Accuracy: 0.6623, Precision: 0.9822, F1: 0.0419\n",
      "Train Accuracy: 0.7273, Precision: 0.9876, F1: 0.0383\n",
      "Train Accuracy: 0.6377, Precision: 0.9849, F1: 0.0324\n",
      "Train Accuracy: 0.7078, Precision: 0.9867, F1: 0.0377\n",
      "Train Accuracy: 0.6727, Precision: 0.9894, F1: 0.0259\n",
      "Train Accuracy: 0.6766, Precision: 0.9838, F1: 0.0404\n",
      "Train Accuracy: 0.6390, Precision: 0.9836, F1: 0.0354\n",
      "Train Accuracy: 0.7156, Precision: 0.9858, F1: 0.0417\n",
      "Validation Accuracy: 0.6987, Precision: 0.9857, F1: 0.0392\n",
      "Validation Accuracy: 0.6935, Precision: 0.9861, F1: 0.0372\n",
      "Train Loss: 4.1394, Validation Loss: 3.9266\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 18 -------------------------\n",
      "Train Accuracy: 0.7364, Precision: 0.9861, F1: 0.0446\n",
      "Train Accuracy: 0.6909, Precision: 0.9845, F1: 0.0409\n",
      "Train Accuracy: 0.6740, Precision: 0.9845, F1: 0.0383\n",
      "Train Accuracy: 0.7026, Precision: 0.9858, F1: 0.0393\n",
      "Train Accuracy: 0.6688, Precision: 0.9849, F1: 0.0364\n",
      "Train Accuracy: 0.6312, Precision: 0.9873, F1: 0.0267\n",
      "Train Accuracy: 0.7052, Precision: 0.9866, F1: 0.0376\n",
      "Train Accuracy: 0.6299, Precision: 0.9832, F1: 0.0351\n",
      "Validation Accuracy: 0.6870, Precision: 0.9851, F1: 0.0388\n",
      "Validation Accuracy: 0.7052, Precision: 0.9877, F1: 0.0345\n",
      "Train Loss: 4.1168, Validation Loss: 3.9045\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 19 -------------------------\n",
      "Train Accuracy: 0.7299, Precision: 0.9887, F1: 0.0352\n",
      "Train Accuracy: 0.6961, Precision: 0.9855, F1: 0.0391\n",
      "Train Accuracy: 0.7039, Precision: 0.9826, F1: 0.0486\n",
      "Train Accuracy: 0.6532, Precision: 0.9827, F1: 0.0395\n",
      "Train Accuracy: 0.6545, Precision: 0.9843, F1: 0.0360\n",
      "Train Accuracy: 0.7000, Precision: 0.9880, F1: 0.0329\n",
      "Train Accuracy: 0.6104, Precision: 0.9874, F1: 0.0245\n",
      "Train Accuracy: 0.6909, Precision: 0.9837, F1: 0.0430\n",
      "Validation Accuracy: 0.6805, Precision: 0.9867, F1: 0.0337\n",
      "Validation Accuracy: 0.7117, Precision: 0.9863, F1: 0.0396\n",
      "Train Loss: 4.0914, Validation Loss: 3.8828\n",
      "--------------------------------------------------\n",
      "------------------------- Epoch 20 -------------------------\n",
      "Train Accuracy: 0.7104, Precision: 0.9862, F1: 0.0396\n",
      "Train Accuracy: 0.7156, Precision: 0.9876, F1: 0.0363\n",
      "Train Accuracy: 0.6571, Precision: 0.9837, F1: 0.0378\n",
      "Train Accuracy: 0.7039, Precision: 0.9852, F1: 0.0413\n",
      "Train Accuracy: 0.6143, Precision: 0.9862, F1: 0.0272\n",
      "Train Accuracy: 0.6506, Precision: 0.9860, F1: 0.0315\n",
      "Train Accuracy: 0.6623, Precision: 0.9839, F1: 0.0379\n",
      "Train Accuracy: 0.7247, Precision: 0.9838, F1: 0.0494\n",
      "Validation Accuracy: 0.6922, Precision: 0.9853, F1: 0.0390\n",
      "Validation Accuracy: 0.7000, Precision: 0.9870, F1: 0.0358\n",
      "Train Loss: 4.0681, Validation Loss: 3.8618\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import BartTokenizer  # import BartTokenizer\n",
    "\n",
    "# initialize the BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Positional encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)\n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_tokens, dim_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(dim_model=dim_model, dropout_p=dropout_p, max_len=5000)\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model, \n",
    "            nhead=num_heads, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            num_decoder_layers=num_decoder_layers, \n",
    "            dropout=dropout_p, \n",
    "            batch_first=True  # setting this to True for nested tensors, error otherwise\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "\n",
    "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        mask = torch.tril(torch.ones(size, size) == 1)\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        return (matrix == pad_token)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(num_tokens=tokenizer.vocab_size, dim_model=8, num_heads=8, num_encoder_layers=12, num_decoder_layers=12, dropout_p=0.1).to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "def untokenize_smiles(token_ids):\n",
    "    #remove autogenerated padding tokens\n",
    "    token_ids = [token_id for token_id in token_ids if token_id != tokenizer.pad_token_id]\n",
    "    \n",
    "    # convert token ids back to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "    # tokens into a string \n",
    "    smiles = tokenizer.convert_tokens_to_string(tokens)\n",
    "    \n",
    "    return smiles\n",
    "\n",
    "def calculate_metrics(predictions, targets, average='macro'):    \n",
    "    # flatten the lists of token IDs to compare tokens at each position in the sequence\n",
    "    predictions_flat = [item for sublist in predictions for item in sublist]\n",
    "    targets_flat = [item for sublist in targets for item in sublist]\n",
    "\n",
    "    # token-level comparison\n",
    "    accuracy = accuracy_score(targets_flat, predictions_flat)\n",
    "\n",
    "    # token-level comparisons\n",
    "    precision = precision_score(targets_flat, predictions_flat, average=average, zero_division=1)\n",
    "    f1 = f1_score(targets_flat, predictions_flat, average=average, zero_division=1)\n",
    "\n",
    "    return accuracy, precision, f1\n",
    "\n",
    "def train_loop(model, opt, loss_fn, train_dataloader, tokenizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        X, y = batch\n",
    "        opt.zero_grad()\n",
    "\n",
    "        pred = model(X, y)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        # reshape pred to [batch_size * seq_len, vocab_size] \n",
    "        pred_flat = pred.view(-1, pred.size(-1))  # flatten sequence and vocab dimensions\n",
    "        y_flat = y.view(-1)  # flatten target sequence\n",
    "\n",
    "        # loss cross entropy expects input: [batch_size * seq_len, vocab_size] targets: [batch_size * seq_len])\n",
    "        loss = loss_fn(pred_flat, y_flat)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # pred token ids\n",
    "        pred_token_ids = pred.argmax(dim=-1).tolist()\n",
    "\n",
    "        # tokenized seq metrics\n",
    "        accuracy, precision, f1 = calculate_metrics(pred_token_ids, y.tolist())\n",
    "        print(f\"Train Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "        # store preds and targets\n",
    "        all_predictions.extend(pred_token_ids)\n",
    "        all_targets.extend(y.tolist())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_dataloader), all_predictions, all_targets\n",
    "\n",
    "def val_loop(model, loss_fn, val_dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            X, y = batch\n",
    "            pred = model(X, y)\n",
    "\n",
    "            pred_flat = pred.view(-1, pred.size(-1))  \n",
    "            y_flat = y.view(-1)  \n",
    "\n",
    "            loss = loss_fn(pred_flat, y_flat)\n",
    "\n",
    "            pred_token_ids = pred.argmax(dim=-1).tolist()\n",
    "\n",
    "            accuracy, precision, f1 = calculate_metrics(pred_token_ids, y.tolist())\n",
    "            print(f\"Validation Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "            all_predictions.extend(pred_token_ids)\n",
    "            all_targets.extend(y.tolist())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_dataloader), all_predictions, all_targets\n",
    "\n",
    "def fit(model, opt, loss_fn, train_loader, val_loader, epochs, tokenizer):\n",
    "    train_loss_list = []\n",
    "    validation_loss_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "\n",
    "        # train \n",
    "        train_loss, train_preds, train_targets = train_loop(model, opt, loss_fn, train_loader, tokenizer)\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "        # validation \n",
    "        val_loss, val_preds, val_targets = val_loop(model, loss_fn, val_loader, tokenizer)\n",
    "        validation_loss_list.append(val_loss)\n",
    "\n",
    "        # save generated SMILES\n",
    "        #generated_smiles = [untokenize_smiles(p) for p in val_preds]  \n",
    "        #smiles_df = pd.DataFrame({'Generated_SMILES': generated_smiles})\n",
    "        #smiles_df.to_csv(f'generated_smiles_epoch_{epoch+1}.csv', index=False)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "    return train_loss_list, validation_loss_list\n",
    "\n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_loader, test_loader, 20, tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
