{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Autoencoder \n",
    "\n",
    "Replicating SMILES autoencoder in this article:  \n",
    "https://www.nature.com/articles/s42004-023-00932-3#Sec19  \n",
    "\n",
    "Supplementary material for above paper:  \n",
    "https://static-content.springer.com/esm/art%3A10.1038%2Fs42004-023-00932-3/MediaObjects/42004_2023_932_MOESM1_ESM.pdf  \n",
    "\n",
    "Model architecure and training details from above supplementary material:   \n",
    "\n",
    "• Bidirectional GRU with an encoder-decoder architecture and  \n",
    "– number of encoder and decoder layers: 3  \n",
    "– hidden dimension for encoder and decoder: 512  \n",
    "– Dimensionality of the embedding space: 512  \n",
    "– nonlinearity: tanh  \n",
    "\n",
    "Training Hyperparameters:  \n",
    "• Batch size: 256  \n",
    "• Learning rate: 0.0001  \n",
    "\n",
    "Training - The autoencoder is trained as a translation model by translating from a randomized SMILES version of a molecule\n",
    "to its canonical version. The model is trained on 135M molecules from Pubchem and ZINC12 datasets.\n",
    "The architecture of the translation model and latent dimension of 512 is similar to the one used in Winter et al.\n",
    "In order to make the learnt representations more meaningful, we also jointly trained a regression model to predict some\n",
    "molecular properties that can be calculated using the molecular structure. The regression model uses two fully connected layers\n",
    "with dimensions 512 and 128 and ReLU non-linearity. The properties that are predicted are: logP, molar refractivity, number\n",
    "of valence electrons, number of hydrogen bond donors and acceptors, Balaban’s J value, topological polar surface area, drug\n",
    "likeliness (QED) and synthetic accessibility (SA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT NECESSARY PACKAGES\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of a string in the input random 'smiles' column is: 74\n",
      "The maximum length of a string in the output canon 'smiles' column is: 77\n",
      "Number of SMILES sequences for input random dataframe: 643168\n",
      "Number of SMILES sequences for output canonical dataframe: 643168\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATASETS AND PAD SMILES STRINGS TO LEN=77\n",
    "\n",
    "# load dataset\n",
    "in_random_smiles_df = pd.read_csv('smiles.csv')     # smiles.csv file with 706863 rows\n",
    "out_canon_smiles_df = pd.read_csv('sanitized_smiles.csv')       # sanitized_smiles.csv file with 706863 rows\n",
    "out_canon_smiles_df.rename(columns={'SMILES': 'smiles'}, inplace=True)\n",
    "\n",
    "# calculate the length of each string in the 'smiles' column\n",
    "in_random_smiles_df['length'] = in_random_smiles_df['smiles'].apply(len)\n",
    "out_canon_smiles_df['length'] = out_canon_smiles_df['smiles'].apply(len)\n",
    "\n",
    "# filter dataframes to only keep input random SMILES with length 74 or less\n",
    "in_filtered_random_smiles_df = in_random_smiles_df[in_random_smiles_df['length'] <= 74]\n",
    "out_filtered_canon_smiles_df = out_canon_smiles_df.loc[in_filtered_random_smiles_df.index]\n",
    "\n",
    "# filter dataframes to only keep output random SMILES with length 77 or less\n",
    "out_filtered_canon_smiles_df = out_filtered_canon_smiles_df[out_filtered_canon_smiles_df['length'] <= 77]\n",
    "in_filtered_random_smiles_df = in_filtered_random_smiles_df.loc[out_filtered_canon_smiles_df.index]\n",
    "\n",
    "# pad the strings in the 'smiles' column to the desired length using \" \"\n",
    "max_length = 77\n",
    "\n",
    "in_filtered_random_smiles_df['smiles_padded'] = in_filtered_random_smiles_df['smiles'].apply(lambda x: x.ljust(max_length, ' '))\n",
    "out_filtered_canon_smiles_df['smiles_padded'] = out_filtered_canon_smiles_df['smiles'].apply(lambda x: x.ljust(max_length, ' '))\n",
    "\n",
    "# Find the maximum length\n",
    "max_length_random = in_filtered_random_smiles_df['length'].max()\n",
    "max_length_canon = out_filtered_canon_smiles_df['length'].max()\n",
    "\n",
    "# Print the maximum length\n",
    "print(f\"The maximum length of a string in the input random 'smiles' column is: {max_length_random}\")\n",
    "print(f\"The maximum length of a string in the output canon 'smiles' column is: {max_length_canon}\")\n",
    "\n",
    "print(f\"Number of SMILES sequences for input random dataframe: {len(in_filtered_random_smiles_df)}\")\n",
    "print(f\"Number of SMILES sequences for output canonical dataframe: {len(out_filtered_canon_smiles_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 64\n",
      "Shape of input SMILES tensor: torch.Size([643168, 77])\n",
      "Shape of output SMILES tensor: torch.Size([643168, 77])\n",
      "Shape of sampled input SMILES tensor: torch.Size([5, 77])\n",
      "Shape of sampled output SMILES tensor: torch.Size([5, 77])\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZE SMILES STRINGS (CONVERT STRING OF CHARACTERS TO LIST OF FLOATS), CONVERT TO INPUT/OUTPUT TENSORS, SAMPLE 5 SMILES SEQUENCES\n",
    "\n",
    "# Convert SMILES strings to list of characters tokens\n",
    "in_filtered_random_smiles_df['smiles_tokenized_lists'] = in_filtered_random_smiles_df['smiles_padded'].apply(lambda x: list(x))\n",
    "out_filtered_canon_smiles_df['smiles_tokenized_lists'] = out_filtered_canon_smiles_df['smiles_padded'].apply(lambda x: list(x))\n",
    "\n",
    "# combines each list of SMILES characters into one list\n",
    "flattened_list_all_random_smiles = [item for sublist in in_filtered_random_smiles_df['smiles_tokenized_lists'] for item in sublist]\n",
    "flattened_list_all_canon_smiles = [item for sublist in out_filtered_canon_smiles_df['smiles_tokenized_lists'] for item in sublist]\n",
    "\n",
    "# get unique characters\n",
    "unique_characters_random = set(flattened_list_all_random_smiles)\n",
    "unique_characters_canon = set(flattened_list_all_canon_smiles)\n",
    "\n",
    "# convert back to list\n",
    "unique_characters_random = list(unique_characters_random)\n",
    "unique_characters_canon = list(unique_characters_canon)\n",
    "\n",
    "# make one unique characters list\n",
    "unique_characters = unique_characters_random + unique_characters_canon\n",
    "unique_characters = set(unique_characters)\n",
    "unique_characters = list(unique_characters)\n",
    "print(f\"Number of unique characters: {len(unique_characters)}\")\n",
    "\n",
    "# mapping from characters to integers\n",
    "char_to_int = {char: i for i, char in enumerate(unique_characters)}\n",
    "\n",
    "# convert SMILES tokenized lists into integer lists\n",
    "int_lists_random_smiles = [\n",
    "    [char_to_int[char] for char in sublist]\n",
    "    for sublist in in_filtered_random_smiles_df['smiles_tokenized_lists']\n",
    "]\n",
    "\n",
    "int_lists_canon_smiles = [\n",
    "    [char_to_int[char] for char in sublist]\n",
    "    for sublist in out_filtered_canon_smiles_df['smiles_tokenized_lists']\n",
    "]\n",
    "\n",
    "in_filtered_random_smiles_df['smiles_integer_lists'] = int_lists_random_smiles\n",
    "out_filtered_canon_smiles_df['smiles_integer_lists'] = int_lists_canon_smiles\n",
    "\n",
    "# convert the tokenized sequences to tensors\n",
    "int_lists_random_smiles = in_filtered_random_smiles_df['smiles_integer_lists'].tolist()\n",
    "int_lists_canon_smiles = out_filtered_canon_smiles_df['smiles_integer_lists'].tolist()\n",
    "\n",
    "in_smiles_tensor = torch.tensor(int_lists_random_smiles, dtype=torch.float32)\n",
    "out_smiles_tensor = torch.tensor(int_lists_canon_smiles, dtype=torch.float32)\n",
    "\n",
    "print(f\"Shape of input SMILES tensor: {in_smiles_tensor.shape}\")\n",
    "print(f\"Shape of output SMILES tensor: {out_smiles_tensor.shape}\")\n",
    "\n",
    "# create random sample of tensors\n",
    "\n",
    "# five random indices between 0 and 643167\n",
    "random_indices = torch.randint(0, 643167, (5,))\n",
    "\n",
    "# take the random sample of five rows from each tensor\n",
    "sample_in_smiles_tensor = in_smiles_tensor[random_indices]\n",
    "sample_out_smiles_tensor = out_smiles_tensor[random_indices]\n",
    "\n",
    "# Print shape of sampled rows\n",
    "print(f\"Shape of sampled input SMILES tensor: {sample_in_smiles_tensor.shape}\")\n",
    "print(f\"Shape of sampled output SMILES tensor: {sample_out_smiles_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled input SMILES tensor: tensor([[ 1.,  1., 50.,  1., 22.,  0., 50., 36.,  1.,  2., 16.,  2., 23., 23.,\n",
      "         22., 23., 29., 40.,  2., 23., 22.,  2., 29., 36., 23., 32., 23., 23.,\n",
      "         22., 13., 36., 23., 23., 23., 32., 13., 36., 23., 16., 23., 46., 23.,\n",
      "         23., 23., 23., 23., 46., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17.],\n",
      "        [50.,  0.,  1., 22.,  1.,  1., 37., 23., 16., 23., 23., 23., 22., 59.,\n",
      "         20., 36., 23., 23., 16., 36., 23., 16., 23., 23., 23., 22., 50., 36.,\n",
      "         23., 23., 16., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17.],\n",
      "        [50.,  0.,  1., 22.,  1., 50., 23., 16., 23., 23., 23., 23., 22., 52.,\n",
      "          1., 22.,  0., 50., 36., 23., 29., 23., 23., 23., 56., 29., 36., 23.,\n",
      "         16., 36., 23., 16., 23., 23., 23., 22., 13., 36., 23., 23., 16., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17.],\n",
      "        [ 1., 23., 16.,  2.,  2., 22., 23., 29., 23., 23., 23., 23., 22., 23.,\n",
      "         29., 36.,  1., 32.,  0., 52., 52.,  1., 22.,  0., 50., 36., 52., 32.,\n",
      "         36., 23., 46.,  1., 22.,  0., 50., 36., 52., 22.,  1.,  1., 23., 16.,\n",
      "         46., 36., 23.,  4., 23., 23., 23., 22., 23., 23.,  4., 36., 23., 57.,\n",
      "         23., 23., 23., 23., 23., 57.,  1., 52., 21.,  1.,  1.,  1.,  1., 21.,\n",
      "         17., 17., 17., 17., 17., 17., 17.],\n",
      "        [50.,  0.,  1., 22.,  1., 50.,  1., 22.,  0., 50., 36., 55.,  1.,  0.,\n",
      "          1., 55., 23., 16., 23., 23., 23., 23., 23., 16., 36., 52., 23., 16.,\n",
      "         23., 23., 23.,  2., 23., 16.,  1., 15., 17., 17., 17., 17., 17., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "         17., 17., 17., 17., 17., 17., 17.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sampled input SMILES tensor: {sample_in_smiles_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bidirectional_GRU_AE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=3, embedding_dim=10):\n",
    "        super(bidirectional_GRU_AE, self).__init__()\n",
    "\n",
    "        # embedding layer to convert integer indices to dense float vectors\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "\n",
    "        self.encoder = nn.GRU(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            bidirectional=True,\n",
    "            # input and output tensors are (batch_size, seq_len) format\n",
    "            batch_first = True      \n",
    "        )\n",
    "\n",
    "        self.decoder = nn.GRU(\n",
    "            # for bidirectional GRU, input size is doubled (each hidden layer as forward and backward state)\n",
    "            input_size=hidden_size * 2,     \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            bidirectional=True, \n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "        # fully connected layer\n",
    "        # passing through Tanh activation function (self.tanh = nn.Tanh) for nonlinearity\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, input_size),  # Linear layer\n",
    "            nn.Tanh()  # Tanh activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        # embed input integer indices to floating point vectors\n",
    "        # after embedding, shape of x becomes (batch size, sequence length, embedding dimensions)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # encoder_output shape is (batch size, sequence length, hidden_size * 2)\n",
    "        # hidden shape is (num_layers * 2, batch size, hidden_size)\n",
    "        encoder_output, hidden = self.encoder(x)\n",
    "        \n",
    "        # for bidirectional GRU, there are two separate hidden states for each hidden layer\n",
    "        # forward state goes from start to end of sequence and backward state goes from end to start of sequence\n",
    "        # we can concatenate forward and backward directions from last hidden layers or pass the hidden layers to the decoder directly\n",
    "        # we are currently passing hidden layers to decoder directly\n",
    "\n",
    "        # decoding\n",
    "        # encoder_output has shape (batch size, sequence length, hidden_size * 2)\n",
    "        # decoder output has shape (batch size, sequence length, hidden_size * 2)\n",
    "        decoder_output, _ = self.decoder(encoder_output, hidden)\n",
    "        print(f\"Output shape after decoder: {decoder_output.shape}\")\n",
    "\n",
    "        # pass decoder output through the fully connected layer\n",
    "        # output has shape of (batch size, sequence length, vocab size) = (5, 77, 64)\n",
    "        output = self.fc(decoder_output)\n",
    "\n",
    "        print(f\"Output shape after fully connected layer: {output.shape}\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after decoder: torch.Size([5, 77, 20])\n",
      "Output shape after fully connected layer: torch.Size([5, 77, 64])\n",
      "input shape: torch.Size([5, 77])\n",
      "output shape: torch.Size([385, 64])\n",
      "target shape: torch.Size([385])\n",
      "Epoch [1/10], Loss: 4.2592\n",
      "Output shape after decoder: torch.Size([5, 77, 20])\n",
      "Output shape after fully connected layer: torch.Size([5, 77, 64])\n",
      "input shape: torch.Size([5, 77])\n",
      "output shape: torch.Size([385, 64])\n",
      "target shape: torch.Size([385])\n",
      "Epoch [2/10], Loss: 4.2566\n",
      "Output shape after decoder: torch.Size([5, 77, 20])\n",
      "Output shape after fully connected layer: torch.Size([5, 77, 64])\n",
      "input shape: torch.Size([5, 77])\n",
      "output shape: torch.Size([385, 64])\n",
      "target shape: torch.Size([385])\n",
      "Epoch [3/10], Loss: 4.2540\n",
      "Output shape after decoder: torch.Size([5, 77, 20])\n",
      "Output shape after fully connected layer: torch.Size([5, 77, 64])\n",
      "input shape: torch.Size([5, 77])\n",
      "output shape: torch.Size([385, 64])\n",
      "target shape: torch.Size([385])\n",
      "Epoch [4/10], Loss: 4.2514\n",
      "Output shape after decoder: torch.Size([5, 77, 20])\n",
      "Output shape after fully connected layer: torch.Size([5, 77, 64])\n",
      "input shape: torch.Size([5, 77])\n",
      "output shape: torch.Size([385, 64])\n",
      "target shape: torch.Size([385])\n",
      "Epoch [5/10], Loss: 4.2488\n",
      "Output shape after decoder: torch.Size([5, 77, 20])\n",
      "Output shape after fully connected layer: torch.Size([5, 77, 64])\n",
      "input shape: torch.Size([5, 77])\n",
      "output shape: torch.Size([385, 64])\n",
      "target shape: torch.Size([385])\n",
      "Epoch [6/10], Loss: 4.2462\n",
      "Output shape after decoder: torch.Size([5, 77, 20])\n",
      "Output shape after fully connected layer: torch.Size([5, 77, 64])\n",
      "input shape: torch.Size([5, 77])\n",
      "output shape: torch.Size([385, 64])\n",
      "target shape: torch.Size([385])\n",
      "Epoch [7/10], Loss: 4.2436\n",
      "Output shape after decoder: torch.Size([5, 77, 20])\n",
      "Output shape after fully connected layer: torch.Size([5, 77, 64])\n",
      "input shape: torch.Size([5, 77])\n",
      "output shape: torch.Size([385, 64])\n",
      "target shape: torch.Size([385])\n",
      "Epoch [8/10], Loss: 4.2411\n",
      "Output shape after decoder: torch.Size([5, 77, 20])\n",
      "Output shape after fully connected layer: torch.Size([5, 77, 64])\n",
      "input shape: torch.Size([5, 77])\n",
      "output shape: torch.Size([385, 64])\n",
      "target shape: torch.Size([385])\n",
      "Epoch [9/10], Loss: 4.2385\n",
      "Output shape after decoder: torch.Size([5, 77, 20])\n",
      "Output shape after fully connected layer: torch.Size([5, 77, 64])\n",
      "input shape: torch.Size([5, 77])\n",
      "output shape: torch.Size([385, 64])\n",
      "target shape: torch.Size([385])\n",
      "Epoch [10/10], Loss: 4.2359\n",
      "[4.259207248687744, 4.256603240966797, 4.254003524780273, 4.251407146453857, 4.248815059661865, 4.2462263107299805, 4.243641376495361, 4.24105978012085, 4.238482475280762, 4.235907554626465]\n"
     ]
    }
   ],
   "source": [
    "# train bidirectional GRU AE model\n",
    "\n",
    "# Hyperparameters\n",
    "seq_len = 77\n",
    "input_size = 64     # vocabulary size  - number of unique chars in SMILES  \n",
    "embed_dim = 10     \n",
    "hidden_dim = 10    \n",
    "num_layers = 3\n",
    "batch_size = 5\n",
    "learning_rate = 0.0001\n",
    "#dropout = 0.1\n",
    "\n",
    "# training data\n",
    "# shape: (batch_size, seq_len) = (5, 77)\n",
    "input = sample_in_smiles_tensor.long()\n",
    "# shape: (batch_size, seq_len) = (5, 77)\n",
    "target = sample_out_smiles_tensor.long()\n",
    "\n",
    "# instantiate model\n",
    "model = bidirectional_GRU_AE(input_size, hidden_dim, num_layers, embed_dim)\n",
    "\n",
    "# optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=17)  # ignore \" \" padding indices: ignore_index=17\n",
    "\n",
    "# training loop\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "\n",
    "    # zeros out the gradients before computing the gradients for the current batch\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(input)\n",
    "\n",
    "    # to compute loss, we flatten the output tensor from (batch_size, seq_len, vocab_size) to (batch_size * seq_len, vocab_size) \n",
    "    # loss function (CrossEntropyLoss) operates over each token position in the sequence\n",
    "\n",
    "    # output shape after flattening is (385, 64)\n",
    "    # 385 is flattened batch and sequence length (5*77 = 385 total sequence positions)\n",
    "    # for each of those positions, there are 64 possible characters/classes\n",
    "    # for each of those 385 positions, the model outputs a probability distribution over 64 possible characters\n",
    "\n",
    "    # flatten output to (batch_size * seq_len, vocab_size) = (5 * 77, 64) = (385, 64)\n",
    "    output = output.contiguous().view(-1, input_size)  \n",
    "\n",
    "    # flatten target to (batch_size * seq_len = 385)\n",
    "    target = target.contiguous().view(-1) \n",
    "\n",
    "    print(f\"input shape: {input.shape}\")\n",
    "    print(f\"output shape: {output.shape}\")\n",
    "    print(f\"target shape: {target.shape}\")\n",
    "\n",
    "    # compute the loss (cross-entropy) for each epoch\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
