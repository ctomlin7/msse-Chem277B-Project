{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectra Encoder: Transformer\n",
    "\n",
    "Primary reference: https://www.nature.com/articles/s42004-023-00932-3#Sec19\n",
    "\n",
    "Using this paper as a framework, the purpose of this transformer is to take input GC-MS spectral data and output embeddings to be passed to the SMILES decoder. The reference used images of GC-MS data and implemented a CNN; we intend to use a transformer instead.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplemental references:\n",
    "https://jalammar.github.io/illustrated-transformer/ (Illustrated overview of Transformer function)\n",
    "\n",
    "https://nlp.seas.harvard.edu/2018/04/03/attention.html (Harvard coding annotation of original Transformation paper)\n",
    "\n",
    "https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch (Datacamp Transformer tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook overview:\n",
    "1. Define model building blocks\n",
    "2. Encoding\n",
    "3. Decoding\n",
    "4. Training\n",
    "5. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# finding unique characters in the SMILES column of training data \n",
    "\n",
    "unique_characters = set() \n",
    "\n",
    "with open('dataset/filtered_gc_spec.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)  \n",
    "    for row in reader:\n",
    "        for char in row[\"SMILES\"]:\n",
    "            unique_characters.add(char)  # Add each character to the set\n",
    "\n",
    "print(len(unique_characters))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517627\n"
     ]
    }
   ],
   "source": [
    "# finding unique tuples in the spectral training data\n",
    "unique_tuples = set()  \n",
    "\n",
    "with open('dataset/filtered_gc_spec.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        spectrum_data = row[\"Spectrum\"]\n",
    "        tuples = spectrum_data.split() \n",
    "        for tup in tuples:\n",
    "            if ':' in tup and tup.count(':') == 1: \n",
    "                unique_tuples.add(tup)\n",
    "\n",
    "print(len(unique_tuples))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to go from a \"vocabulary\" of 517627 unique tuples to 45 unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input and output datasets for training\n",
    "\n",
    "# load dataset\n",
    "data = pd.read_csv('dataset/filtered_gc_spec.csv')\n",
    "input_MS = pd.Series(data[\"Spectrum\"][:5])\n",
    "output_SMILES = pd.Series(data[\"SMILES\"][:5])\n",
    "\n",
    "assert len(input_MS)==len(output_SMILES) #sanity check to ensure correct loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GC-MS Spectra for input: 5\n",
      "Number of SMILES sequences for output: 5\n"
     ]
    }
   ],
   "source": [
    "#filter input by length of SMILES (<77 as per SMILES encoder)\n",
    "\n",
    "output_SMILES_filtered = output_SMILES[output_SMILES.str.len() < 77]\n",
    "\n",
    "#filter input by the same indices\n",
    "input_MS_filtered = input_MS.loc[output_SMILES_filtered.index]\n",
    "\n",
    "assert len(input_MS_filtered)==len(output_SMILES_filtered) #sanity check to ensure correct filtering\n",
    "\n",
    "print(f\"Number of GC-MS Spectra for input: {len(input_MS_filtered)}\")\n",
    "print(f\"Number of SMILES sequences for output: {len(output_SMILES_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the SMILES data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepchem\n",
      "  Downloading deepchem-2.8.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: joblib in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from deepchem) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from deepchem) (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from deepchem) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from deepchem) (1.2.2)\n",
      "Requirement already satisfied: sympy in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from deepchem) (1.13.1)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from deepchem) (1.14.1)\n",
      "Requirement already satisfied: rdkit in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from deepchem) (2023.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from pandas->deepchem) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: Pillow in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from rdkit->deepchem) (10.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from scikit-learn->deepchem) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from sympy->deepchem) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
      "Downloading deepchem-2.8.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: deepchem\n",
      "Successfully installed deepchem-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "2024-12-01 19:19:06.842836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733109546.953096 1786219 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733109546.988163 1786219 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 19:19:07.245322: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m902.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, tokenizers, transformers\n",
      "Successfully installed safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (0.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from tokenizers) (0.24.6)\n",
      "Requirement already satisfied: filelock in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/marieanand/miniconda3/envs/msse-python/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_list = output_SMILES_filtered.tolist()\n",
    "\n",
    "len(smiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816578/396997768.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# Requriments - transformers, tokenizers\n",
    "# Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset.\n",
    "# The vocab may be expanded in the near future\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import pkg_resources\n",
    "from typing import List\n",
    "from transformers import BertTokenizer\n",
    "from logging import getLogger\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\"\"\"\n",
    "SMI_REGEX_PATTERN: str\n",
    "    SMILES regex pattern for tokenization. Designed by Schwaller et. al.\n",
    "\n",
    "References\n",
    "\n",
    ".. [1]  Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A. Hunter, Costas Bekas, and Alpha A. Lee\n",
    "        ACS Central Science 2019 5 (9): Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction\n",
    "        1572-1583 DOI: 10.1021/acscentsci.9b00576\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "SMI_REGEX_PATTERN = r\"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|\n",
    "#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "\n",
    "# add vocab_file dict\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "\n",
    "\n",
    "class SmilesTokenizer(BertTokenizer):\n",
    "    def __init__(self, vocab_file: str = '', max_len: int = 512, **kwargs):\n",
    "        \"\"\"Constructs a SmilesTokenizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_file: str\n",
    "            Path to a SMILES character per line vocabulary file.\n",
    "            Default vocab file is found in deepchem/feat/tests/data/vocab.txt\n",
    "        max_len: int\n",
    "            Maximum length for tokenized sequences.\n",
    "        \"\"\"\n",
    "        self.max_len = max_len\n",
    "\n",
    "        super().__init__(vocab_file=vocab_file, max_len=max_len, **kwargs)\n",
    "        \n",
    "        # take into account special tokens in max length\n",
    "        #self.max_len_single_sentence = self.max_len - 2\n",
    "        #self.max_len_sentences_pair = self.max_len - 3\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocab file at path '{}'.\".format(vocab_file)\n",
    "            )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.highest_unused_index = max(\n",
    "            [i for i, v in enumerate(self.vocab.keys()) if v.startswith(\"[unused\")])\n",
    "        self.ids_to_tokens = collections.OrderedDict(\n",
    "            [(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.basic_tokenizer = BasicSmilesTokenizer()\n",
    "        self.init_kwargs[\"max_len\"] = self.max_len\n",
    "\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "      return len(self.vocab)\n",
    "\n",
    "    @property\n",
    "    def vocab_list(self):\n",
    "      return list(self.vocab.keys())\n",
    "\n",
    "    def _tokenize(self, text: str):\n",
    "      \"\"\"\n",
    "          Tokenize a string into a list of tokens.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          text: str\n",
    "              Input string sequence to be tokenized.\n",
    "          \"\"\"\n",
    "\n",
    "      split_tokens = [token for token in self.basic_tokenizer.tokenize(text)]\n",
    "      return split_tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "      \"\"\"\n",
    "          Converts a token (str/unicode) in an id using the vocab.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          token: str\n",
    "              String token from a larger sequence to be converted to a numerical id.\n",
    "          \"\"\"\n",
    "\n",
    "      return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "      \"\"\"\n",
    "          Converts an index (integer) in a token (string/unicode) using the vocab.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          index: int\n",
    "              Integer index to be converted back to a string-based token as part of a larger sequence.\n",
    "          \"\"\"\n",
    "\n",
    "      return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens: List[str]):\n",
    "      \"\"\" Converts a sequence of tokens (string) in a single string.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          tokens: List[str]\n",
    "              List of tokens for a given string sequence.\n",
    "\n",
    "          Returns\n",
    "          -------\n",
    "          out_string: str\n",
    "              Single string from combined tokens.\n",
    "          \"\"\"\n",
    "\n",
    "      out_string: str = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "      return out_string\n",
    "\n",
    "    def add_special_tokens_ids_single_sequence(self, token_ids: List[int]):\n",
    "      \"\"\"\n",
    "          Adds special tokens to the a sequence for sequence classification tasks.\n",
    "          A BERT sequence has the following format: [CLS] X [SEP]\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "\n",
    "          token_ids: list[int]\n",
    "              list of tokenized input ids. Can be obtained using the encode or encode_plus methods.\n",
    "          \"\"\"\n",
    "\n",
    "      return [self.cls_token_id] + token_ids + [self.sep_token_id]\n",
    "\n",
    "    def add_special_tokens_single_sequence(self, tokens: List[str]):\n",
    "      \"\"\"\n",
    "          Adds special tokens to the a sequence for sequence classification tasks.\n",
    "          A BERT sequence has the following format: [CLS] X [SEP]\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          tokens: List[str]\n",
    "              List of tokens for a given string sequence.\n",
    "\n",
    "          \"\"\"\n",
    "      return [self.cls_token] + tokens + [self.sep_token]\n",
    "\n",
    "    def add_special_tokens_ids_sequence_pair(self, token_ids_0: List[int],\n",
    "                                            token_ids_1: List[int]) -> List[int]:\n",
    "      \"\"\"\n",
    "          Adds special tokens to a sequence pair for sequence classification tasks.\n",
    "          A BERT sequence pair has the following format: [CLS] A [SEP] B [SEP]\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          token_ids_0: List[int]\n",
    "              List of ids for the first string sequence in the sequence pair (A).\n",
    "\n",
    "          token_ids_1: List[int]\n",
    "              List of tokens for the second string sequence in the sequence pair (B).\n",
    "          \"\"\"\n",
    "\n",
    "      sep = [self.sep_token_id]\n",
    "      cls = [self.cls_token_id]\n",
    "\n",
    "      return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def add_padding_tokens(self,\n",
    "                          token_ids: List[int],\n",
    "                          length: int,\n",
    "                          right: bool = True) -> List[int]:\n",
    "      \"\"\"\n",
    "          Adds padding tokens to return a sequence of length max_length.\n",
    "          By default padding tokens are added to the right of the sequence.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          token_ids: list[int]\n",
    "              list of tokenized input ids. Can be obtained using the encode or encode_plus methods.\n",
    "\n",
    "          length: int\n",
    "\n",
    "          right: bool (True by default)\n",
    "\n",
    "          Returns\n",
    "          ----------\n",
    "          token_ids :\n",
    "              list of tokenized input ids. Can be obtained using the encode or encode_plus methods.\n",
    "\n",
    "          padding: int\n",
    "              Integer to be added as padding token\n",
    "\n",
    "          \"\"\"\n",
    "      padding = [self.pad_token_id] * (length - len(token_ids))\n",
    "\n",
    "      if right:\n",
    "        return token_ids + padding\n",
    "      else:\n",
    "        return padding + token_ids\n",
    "\n",
    "    def save_vocabulary(\n",
    "        self, vocab_path: str\n",
    "    ):  # -> tuple[str]: doctest issue raised with this return type annotation\n",
    "      \"\"\"\n",
    "          Save the tokenizer vocabulary to a file.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          vocab_path: obj: str\n",
    "              The directory in which to save the SMILES character per line vocabulary file.\n",
    "              Default vocab file is found in deepchem/feat/tests/data/vocab.txt\n",
    "\n",
    "          Returns\n",
    "          ----------\n",
    "          vocab_file: :obj:`Tuple(str)`:\n",
    "              Paths to the files saved.\n",
    "              typle with string to a SMILES character per line vocabulary file.\n",
    "              Default vocab file is found in deepchem/feat/tests/data/vocab.txt\n",
    "\n",
    "          \"\"\"\n",
    "      index = 0\n",
    "      if os.path.isdir(vocab_path):\n",
    "        vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "      else:\n",
    "        vocab_file = vocab_path\n",
    "      with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "        for token, token_index in sorted(\n",
    "            self.vocab.items(), key=lambda kv: kv[1]):\n",
    "          if index != token_index:\n",
    "            logger.warning(\n",
    "                \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                \" Please check that the vocabulary is not corrupted!\".format(\n",
    "                    vocab_file))\n",
    "            index = token_index\n",
    "          writer.write(token + \"\\n\")\n",
    "          index += 1\n",
    "      return (vocab_file,)\n",
    "\n",
    "\n",
    "class BasicSmilesTokenizer(object):\n",
    "  \"\"\"\n",
    "\n",
    "    Run basic SMILES tokenization using a regex pattern developed by Schwaller et. al. This tokenizer is to be used\n",
    "    when a tokenizer that does not require the transformers library by HuggingFace is required.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
    "    >>> tokenizer = BasicSmilesTokenizer()\n",
    "    >>> print(tokenizer.tokenize(\"CC(=O)OC1=CC=CC=C1C(=O)O\"))\n",
    "    ['C', 'C', '(', '=', 'O', ')', 'O', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', 'C', '(', '=', 'O', ')', 'O']\n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1]  Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A. Hunter, Costas Bekas, and Alpha A. Lee\n",
    "            ACS Central Science 2019 5 (9): Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction\n",
    "            1572-1583 DOI: 10.1021/acscentsci.9b00576\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  def __init__(self, regex_pattern: str = SMI_REGEX_PATTERN):\n",
    "    \"\"\" Constructs a BasicSMILESTokenizer.\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        regex: string\n",
    "            SMILES token regex\n",
    "\n",
    "        \"\"\"\n",
    "    self.regex_pattern = regex_pattern\n",
    "    self.regex = re.compile(self.regex_pattern)\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\" Basic Tokenization of a SMILES.\n",
    "        \"\"\"\n",
    "    tokens = [token for token in self.regex.findall(text)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "  vocab = collections.OrderedDict()\n",
    "  with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "    tokens = reader.readlines()\n",
    "  for index, token in enumerate(tokens):\n",
    "    token = token.rstrip(\"\\n\")\n",
    "    vocab[token] = index\n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'vocab.txt'\n",
    "tokenizer = SmilesTokenizer(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to pad the sequence if they're not the max length (6))\n",
    "\n",
    "max_length = 64\n",
    "\n",
    "def pad_seq(tokens, max_length):\n",
    "    return tokens + [0] * (max_length - len(tokens))  # padding\n",
    "\n",
    "\n",
    "#tokenize SMILES data\n",
    "tokenized_smiles = [tokenizer.encode(smiles) for smiles in smiles_list]\n",
    "\n",
    "#pad all entries\n",
    "padded_smiles = [pad_seq(tokens, max_length) for tokens in tokenized_smiles]\n",
    "\n",
    "#convert to tensor\n",
    "smiles_tensor = torch.tensor(padded_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64])\n"
     ]
    }
   ],
   "source": [
    "print(smiles_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the spectrum data to tensors\n",
    "\n",
    "\n",
    "def spec_2_tensor(spectrum: str, max_length: 64):\n",
    "    #takes in a string that has the spectrum and the max length\n",
    "    spectrum_tuples = [(float(mz), float(intensity)) for mz, intensity in (item.split(\":\") for item in spectrum.split())]\n",
    "    return spectrum_tuples[:max_length] + [(0, 0)] * (max_length - len(spectrum_tuples))  # Padding\n",
    "\n",
    "input_MS_data = input_MS_filtered.apply(lambda x: spec_2_tensor(x, 64))\n",
    "\n",
    "ms_tensor = torch.tensor(input_MS_data.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64, 2])\n"
     ]
    }
   ],
   "source": [
    "print(ms_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create batches\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "# create a dataset and DataLoader\n",
    "dataset = TensorDataset(ms_tensor, smiles_tensor)\n",
    "batch_size = 8\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "       \n",
    "        # model parameters\n",
    "        self.d_model = d_model # dimensions of the model\n",
    "        self.num_heads = num_heads # number of attention heads\n",
    "        self.d_k = d_model // num_heads # dimension of each head's key, query, and value\n",
    "        \n",
    "        # transformations\n",
    "        self.W_q = nn.Linear(d_model, d_model) # query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # apply mask if necessary\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # softmax to get attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # compute final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff): #inputs - dimensions and inner-layer dimensions\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings class converts input/output tokens to vectors specified by the dimensions of our model\n",
    "#softmax converts the output to probabilities\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positional encoding is used to inject token position info into the input\n",
    "#otherwise transformer has no info about token position in the input sequence\n",
    "#essentially uses offset sin/cos graphs based on position. freq/offset is different for each dimension\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "    # Ensure the dimensions match before adding\n",
    "        if x.size(2) != self.pe.size(2):\n",
    "            raise ValueError(f\"Dimension mismatch: {x.size(2)} != {self.pe.size(2)}\")\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    #adds positional info to the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining encoder layer for class\n",
    "#steps: multiattention, position feed forward, 2x layer normalization, dropout\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads) #self attention mechanism\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff) #attention output appended to original input\n",
    "        self.norm1 = nn.LayerNorm(d_model) #normalization\n",
    "        self.norm2 = nn.LayerNorm(d_model) #normalization\n",
    "        self.dropout = nn.Dropout(dropout) #dropout\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining decoder layer for class\n",
    "#steps: self attention, normalize, cross attention, normalize, feed forward, normalize\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        #initialize with previously designated parameters\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  # src embedding\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)  # tgt embedding layer\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Transformer layers\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])  # encoder layers\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])  # decoder layers\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)  # linear layer to map to target vocab size\n",
    "        self.dropout = nn.Dropout(dropout)  # dropout layer\n",
    "\n",
    "    def generate_mask(self, src, tgt, pad_idx):\n",
    "        # sequence lengths for source and target\n",
    "        src_len = src.size(1)  # source sequence length\n",
    "        tgt_len = tgt.size(1)  # target sequence length\n",
    "\n",
    "        # create source mask (1 for real tokens, 0 for padding tokens)\n",
    "        src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "        # generate nopeak mask for the target sequence (upper triangular matrix)\n",
    "        nopeak_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool()  # Shape: (tgt_len, tgt_len)\n",
    "\n",
    "        # adjust nopeak_mask size to match the target sequence length dynamically\n",
    "        nopeak_mask = nopeak_mask[:tgt_len, :tgt_len]  # ensure nopeak_mask size matches target length\n",
    "\n",
    "        # ensure the tgt_mask has compatible dimensions for broadcasting\n",
    "        tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)  # Shape: (batch_size, 1, seq_len, 1)\n",
    "        tgt_mask = tgt_mask & nopeak_mask.unsqueeze(0).unsqueeze(0)  # Shape: (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, pad_idx=0):\n",
    "        # Generate masks\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt, pad_idx)\n",
    "        \n",
    "        # apply embedding + positional encoding\n",
    "        src_embedded = self.encoder_embedding(src)\n",
    "        src_embedded = self.positional_encoding(src_embedded)  # Add positional encoding to source embeddings\n",
    "        src_embedded = self.dropout(src_embedded)  # Apply dropout to source embeddings\n",
    "\n",
    "        tgt_embedded = self.decoder_embedding(tgt)\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded)  # Add positional encoding to target embeddings\n",
    "        tgt_embedded = self.dropout(tgt_embedded)  # Apply dropout to target embeddings\n",
    "\n",
    "        # Pass through encoder layers\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        # Linear layer to output the final predictions\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "src_vocab_size = 64\n",
    "tgt_vocab_size = max(smiles_tensor.max() + 1, 64)  # ensure vocab size is at least the largest index in smiles_tensor\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048  # From original paper\n",
    "max_seq_len = max(ms_tensor.size(1), smiles_tensor.size(1))\n",
    "dropout = 0.1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(2, src_vocab_size)  # map from 2 features to vocab size\n",
    "ms_tensor_indices = linear_layer(ms_tensor.view(-1, 2))  # flatten for batch processing\n",
    "\n",
    "ms_tensor_indices = ms_tensor_indices.argmax(dim=1)  # select the index with the highest value\n",
    "ms_tensor_indices = ms_tensor_indices.view(5, 64)  # reshape back to (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m smiles_tensor \u001b[38;5;241m=\u001b[39m smiles_tensor\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mms_tensor_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmiles_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[30], line 57\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, pad_idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m src_embedded\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enc_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[0;32m---> 57\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[43menc_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Pass through decoder layers\u001b[39;00m\n\u001b[1;32m     60\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m tgt_embedded\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m---> 14\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output))\n\u001b[1;32m     16\u001b[0m     ff_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[37], line 52\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[1;32m     49\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(Q, K, V, mask)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Combine heads and apply output transformation\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_o(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[0;32mIn[37], line 39\u001b[0m, in \u001b[0;36mMultiHeadAttention.combine_heads\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_heads\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Combine heads back to original shape\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     batch_size, _, seq_length, d_k \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, seq_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "ms_tensor_indices = ms_tensor_indices.to(device)\n",
    "smiles_tensor = smiles_tensor.to(device)\n",
    "\n",
    "ms_tensor_indices = ms_tensor_indices.long()\n",
    "smiles_tensor = smiles_tensor.long()\n",
    "\n",
    "# Forward pass\n",
    "output = transformer(ms_tensor_indices, smiles_tensor, pad_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from example, haven't tested yet\n",
    "\n",
    "transformer.eval()\n",
    "#setting transformer into evaluation mode\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
